{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape (11824, 784)\n",
      "y train shape (11824,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "X = np.concatenate([X[np.where(y==0)[0]], X[np.where(y==1)[0]]], axis=0)\n",
    "y = np.concatenate([np.zeros(sum(y==0)), np.ones(sum(y==1))], axis=0)\n",
    "\n",
    "\n",
    "X = X / 255.0\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "print('X train shape', X_train.shape)\n",
    "print('y train shape', y_train.shape)\n",
    "\n",
    "\n",
    "# define the batch loader of the dataset\n",
    "def batch_loader(X, y, batch, shuffle=True):\n",
    "    num_samples = len(X)\n",
    "    pointer = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            idx = np.random.choice(num_samples, batch)\n",
    "        else:\n",
    "            if pointer + batch <= num_samples:\n",
    "                idx = np.arange(pointer, pointer+batch)\n",
    "                pointer = pointer + batch\n",
    "            else:\n",
    "                pointer = 0\n",
    "                idx = np.arange(pointer, pointer+batch)\n",
    "                pointer = pointer + batch\n",
    "        yield X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class operator(object):\n",
    "    \"\"\"\n",
    "    Operator abstraction\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward operator, reture output from this operator\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        \"\"\"Backward operator, return gradient to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class matmul(operator):\n",
    "    def __init__(self):\n",
    "        super(matmul, self).__init__()\n",
    "\n",
    "    def forward(self, input, weights):\n",
    "        \"\"\"\n",
    "        # Arguments\n",
    "            input: numpy array with shape (batch, in_features)\n",
    "            weights: numpy array with shape (in_features, out_features)\n",
    "\n",
    "        # Returns\n",
    "            output: numpy array with shape(batch, out_features)\n",
    "        \"\"\"\n",
    "        return np.matmul(input, weights)\n",
    "\n",
    "    def backward(self, out_grad, input, weights):\n",
    "        \"\"\"\n",
    "        # Arguments\n",
    "            out_grad: gradient to the forward output of fc layer, with shape (batch, out_features)\n",
    "            input: numpy array with shape (batch, in_features)\n",
    "            weights: numpy array with shape (in_features, out_features)\n",
    "\n",
    "        # Returns\n",
    "            in_grad: gradient to the forward input with same shape as input\n",
    "            w_grad: gradient to weights, with same shape as weights            \n",
    "        \"\"\"\n",
    "        in_grad = np.matmul(out_grad, weights.T)\n",
    "        w_grad = np.matmul(input.T, out_grad)\n",
    "        return in_grad, w_grad\n",
    "\n",
    "\n",
    "class add_bias(operator):\n",
    "    def __init__(self):\n",
    "        super(add_bias, self).__init__()\n",
    "\n",
    "    def forward(self, input, bias):\n",
    "        '''TODO\n",
    "        # Arugments\n",
    "          input: numpy array with shape (batch, in_features)\n",
    "          bias: numpy array with shape (in_features)\n",
    "\n",
    "        # Returns\n",
    "          output: numpy array with shape(batch, in_features)\n",
    "        '''\n",
    "        return input+bias\n",
    "\n",
    "    def backward(self, out_grad, input, bias):\n",
    "        \"\"\"TODO\n",
    "        # Arguments\n",
    "            out_grad: gradient to the forward output of fc layer, with shape (batch, out_features)\n",
    "            input: numpy array with shape (batch, in_features)\n",
    "            bias: numpy array with shape (out_features)\n",
    "        # Returns\n",
    "            in_grad: gradient to the forward input with same shape as input\n",
    "            b_bias: gradient to bias, with same shape as bias\n",
    "        \"\"\"\n",
    "        in_grad = np.matmul(out_grad,np.ones((out_grad.shape[-1],input.shape[-1])))\n",
    "        b_grad = np.matmul(out_grad.T,np.ones(out_grad.shape[0]))\n",
    "        \n",
    "        return in_grad, b_grad\n",
    "\n",
    "\n",
    "class relu(operator):\n",
    "    def __init__(self):\n",
    "        super(relu, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        mask = np.where(input>0,1,0)\n",
    "        output = input*mask\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        mask = np.where(input>0,1,0)\n",
    "        in_grad = out_grad*mask\n",
    "        return in_grad\n",
    "\n",
    "\n",
    "class softmax_cross_entropy(operator):\n",
    "    def __init__(self):\n",
    "        super(softmax_cross_entropy, self).__init__()\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        \"\"\"\n",
    "        # Arguments\n",
    "            input: numpy array with shape (batch, num_class)\n",
    "            labels: numpy array with shape (batch,)\n",
    "            eps: float, precision to avoid overflow\n",
    "\n",
    "        # Returns\n",
    "            output: scalar, average loss\n",
    "            probs: the probability of each category\n",
    "        \"\"\"\n",
    "        # precision to avoid overflow\n",
    "        eps = 1e-12\n",
    "        batch = len(labels)\n",
    "        labels = labels.astype('int')\n",
    "        # convert labels into a matrix format\n",
    "        y = np.zeros(input.shape)\n",
    "        y[np.arange(batch), labels]=1\n",
    "\n",
    "        input_shift = input - np.max(input, axis=1, keepdims=True)\n",
    "        Z = np.sum(np.exp(input_shift), axis=1, keepdims=True)\n",
    "\n",
    "        log_probs = input_shift - np.log(Z+eps)\n",
    "        probs = np.exp(log_probs)\n",
    "        output = -1 * np.sum(log_probs * y) / batch\n",
    "        return output, probs\n",
    "\n",
    "    def backward(self, input, labels):\n",
    "        \"\"\"\n",
    "        # Arguments\n",
    "            input: numpy array with shape (batch, num_class)\n",
    "            labels: numpy array with shape (batch,)\n",
    "            eps: float, precision to avoid overflow\n",
    "\n",
    "        # Returns\n",
    "            in_grad: gradient to forward input of softmax cross entropy, with shape (batch, num_class)\n",
    "        \"\"\"\n",
    "        # precision to avoid overflow\n",
    "        eps = 1e-12\n",
    "        batch = len(labels)\n",
    "        labels = labels.astype('int')\n",
    "        y = np.zeros(input.shape)\n",
    "        y[np.arange(batch), labels]=1\n",
    "\n",
    "        input_shift = input - np.max(input, axis=1, keepdims=True)\n",
    "        Z = np.sum(np.exp(input_shift), axis=1, keepdims=True)\n",
    "        log_probs = input_shift - np.log(Z+eps)\n",
    "        probs = np.exp(log_probs)\n",
    "\n",
    "        in_grad = probs-y\n",
    "\n",
    "        \"\"\"TODO compute the in_grad\"\"\"\n",
    "        return in_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer():\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            lr: float, learnig rate \n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, x, x_grad, iteration):\n",
    "        \"\"\"Update parameters with gradients\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "  \n",
    "\n",
    "class SGD(optimizer):\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            lr: float, learnig rate \n",
    "            momentum: float, the ratio of moments            \n",
    "        \"\"\"\n",
    "        super(SGD, self).__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        self.moments = None         \n",
    "        \n",
    "\n",
    "    def update(self, xs, xs_grads, iteration):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            xs: dictionary, all weights of model\n",
    "            xs_grads: dictionary, gradients to all weights of model, same keys with xs\n",
    "            iteration: int, current iteration number in the whole training process (not in that epoch)\n",
    "\n",
    "        # Returns\n",
    "            new_xs: dictionary, new weights of model\n",
    "        \"\"\"\n",
    "        new_xs = {}        \n",
    "        if not self.moments:\n",
    "            self.moments = {}\n",
    "            for k, v in xs_grads.items():\n",
    "                self.moments[k] = np.zeros(v.shape)        \n",
    "        for k in list(xs.keys()):\n",
    "            \"\"\"TODO update the parameter based on the gradient and momentum\"\"\"\n",
    "            new_xs[k] = xs[k]-self.lr*xs_grads[k]\n",
    "        return new_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_classifier():\n",
    "\n",
    "    def __init__(self, n_in, n_out1, n_out2):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            n_in: the number of input features\n",
    "            n_out1: the output features of first fully connected layer\n",
    "            n_out2: the output features of first fully connected layer\n",
    "\n",
    "        # Returns\n",
    "            new_xs: dictionary, new weights of model\n",
    "        \"\"\"\n",
    "        self.matmul = matmul()\n",
    "        self.add_bias = add_bias()\n",
    "        self.relu = relu()\n",
    "        self.softmax_cross_entropy = softmax_cross_entropy()\n",
    "\n",
    "        # initialize parameters\n",
    "        self.w1 = np.random.normal(loc=0.0, scale=0.1, size=(n_in, n_out1))\n",
    "        self.b1 = np.random.normal(loc=0.0, scale=0.1, size=(n_out1, ))\n",
    "        \n",
    "        # TODO initialize w2 and b2\n",
    "        self.w2 = np.random.normal(loc=0.0, scale=0.1, size=(n_out1, n_out2))\n",
    "        self.b2 = np.random.normal(loc=0.0, scale=0.1, size=(n_out2, ))\n",
    "        \n",
    "        # cacehs to save intermedia results of forward\n",
    "        self.caches = []\n",
    "\n",
    "    def accuracy(self, probs, target):\n",
    "        # probs: probability that each image is labeled as 1\n",
    "        # target: ground truth label\n",
    "        \n",
    "        prediction = probs.argmax(axis=-1)    \n",
    "        acc = np.mean(prediction == target)\n",
    "        return acc * 100\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        param_dict = {\n",
    "            'w1': self.w1,\n",
    "            'b1': self.b1,\n",
    "            'w2': self.w2,\n",
    "            'b2': self.b2,\n",
    "        }\n",
    "        return param_dict\n",
    "\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        # compute the accuracy and loss\n",
    "\n",
    "        caches = [X] # to save intermedia results for backward pass\n",
    "        \n",
    "        # layer 1\n",
    "        out1 = self.matmul.forward(X, self.w1)\n",
    "        caches += [out1]\n",
    "        out1 = self.add_bias.forward(out1, self.b1)\n",
    "        caches += [out1]\n",
    "        out1 = self.relu.forward(out1)\n",
    "        caches += [out1]\n",
    "\n",
    "\n",
    "        # layer 2\n",
    "        out2 = self.matmul.forward(out1, self.w2)\n",
    "        caches += [out2]\n",
    "        out2 = self.add_bias.forward(out2, self.b2)\n",
    "        caches += [out2]\n",
    "\n",
    "\n",
    "        self.caches = caches\n",
    "\n",
    "        # loss\n",
    "        loss, probs = self.softmax_cross_entropy.forward(out2, y)\n",
    "        acc = self.accuracy(probs, y)  \n",
    "\n",
    "        return acc, loss\n",
    "\n",
    "    def backward(self, X, y):\n",
    "\n",
    "\n",
    "        # loss backward\n",
    "        inp = self.caches.pop()\n",
    "        in_grad = self.softmax_cross_entropy.backward(inp, y)\n",
    "\n",
    "        # TODO layer 2 backward\n",
    "        in_grad1,b2_grad = self.add_bias.backward(in_grad,self.caches.pop(),self.b2)\n",
    "        in_grad2,w2_grad = self.matmul.backward(in_grad,self.caches.pop(),self.w2)\n",
    "        relu_grad = self.relu.backward(in_grad2,self.caches.pop())\n",
    "\n",
    "        \n",
    "        # TODO layer 1 backward\n",
    "        in_grad1,b1_grad = self.add_bias.backward(relu_grad,self.caches.pop(),self.b1)\n",
    "        in_grad2,w1_grad = self.matmul.backward(relu_grad,self.caches.pop(),self.w1)\n",
    "        \n",
    "        grad_dict = {\n",
    "            'w1': w1_grad,\n",
    "            'b1': b1_grad,\n",
    "            'w2': w2_grad,\n",
    "            'b2': b2_grad,\n",
    "        }\n",
    "\n",
    "        return grad_dict\n",
    "\n",
    "    \n",
    "    def update(self, new_param_dict):\n",
    "        self.w1 = new_param_dict['w1']\n",
    "        self.b1 = new_param_dict['b1']\n",
    "        self.w2 = new_param_dict['w2']\n",
    "        self.b2 = new_param_dict['b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> epoch 1\n",
      "\t iter 0 \t train acc = 62.50%, train loss = 0.6861\n",
      "\t iter 50 \t train acc = 96.88%, train loss = 0.2724\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.1248\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.1229\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0844\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0758\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.1098\n",
      "\t iter 350 \t train acc = 96.88%, train loss = 0.0789\n",
      "\t avg train acc = 98.19%, avg train loss = 0.1383 | test acc = 99.29%, test loss = 0.0580\n",
      "==> epoch 2\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0452\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0484\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0313\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0422\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0327\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0323\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0794\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0509\n",
      "\t avg train acc = 99.45%, avg train loss = 0.0413 | test acc = 99.42%, test loss = 0.0354\n",
      "==> epoch 3\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0227\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0261\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0176\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0261\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0201\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0205\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0652\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0393\n",
      "\t avg train acc = 99.57%, avg train loss = 0.0273 | test acc = 99.53%, test loss = 0.0272\n",
      "==> epoch 4\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0152\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0180\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0121\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0191\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0144\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0152\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0560\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0325\n",
      "\t avg train acc = 99.64%, avg train loss = 0.0212 | test acc = 99.56%, test loss = 0.0227\n",
      "==> epoch 5\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0115\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0139\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0091\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0151\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0113\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0121\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0492\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0279\n",
      "\t avg train acc = 99.65%, avg train loss = 0.0177 | test acc = 99.59%, test loss = 0.0199\n",
      "==> epoch 6\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0093\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0114\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0072\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0126\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0092\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0101\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0439\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0245\n",
      "\t avg train acc = 99.67%, avg train loss = 0.0153 | test acc = 99.63%, test loss = 0.0180\n",
      "==> epoch 7\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0079\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0097\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0060\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0109\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0078\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0088\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0396\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0219\n",
      "\t avg train acc = 99.69%, avg train loss = 0.0137 | test acc = 99.66%, test loss = 0.0165\n",
      "==> epoch 8\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0068\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0086\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0051\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0096\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0068\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0077\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0361\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0199\n",
      "\t avg train acc = 99.69%, avg train loss = 0.0124 | test acc = 99.66%, test loss = 0.0154\n",
      "==> epoch 9\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0060\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0077\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0044\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0087\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0060\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0069\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0331\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0182\n",
      "\t avg train acc = 99.72%, avg train loss = 0.0114 | test acc = 99.66%, test loss = 0.0144\n",
      "==> epoch 10\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0054\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0070\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0039\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0079\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0054\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0063\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0306\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0168\n",
      "\t avg train acc = 99.73%, avg train loss = 0.0106 | test acc = 99.66%, test loss = 0.0137\n",
      "==> epoch 11\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0050\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0065\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0034\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0073\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0048\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0058\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0284\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0156\n",
      "\t avg train acc = 99.75%, avg train loss = 0.0100 | test acc = 99.70%, test loss = 0.0130\n",
      "==> epoch 12\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0046\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0060\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0031\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0068\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0044\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0054\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0265\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0146\n",
      "\t avg train acc = 99.76%, avg train loss = 0.0094 | test acc = 99.70%, test loss = 0.0125\n",
      "==> epoch 13\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0042\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0057\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0028\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0064\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0041\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0050\n",
      "\t iter 300 \t train acc = 96.88%, train loss = 0.0247\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0137\n",
      "\t avg train acc = 99.76%, avg train loss = 0.0089 | test acc = 99.70%, test loss = 0.0120\n",
      "==> epoch 14\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0039\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0054\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0025\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0060\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0038\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0047\n",
      "\t iter 300 \t train acc = 100.00%, train loss = 0.0231\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0130\n",
      "\t avg train acc = 99.78%, avg train loss = 0.0085 | test acc = 99.70%, test loss = 0.0116\n",
      "==> epoch 15\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0037\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0051\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0023\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0057\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0035\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0044\n",
      "\t iter 300 \t train acc = 100.00%, train loss = 0.0216\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0123\n",
      "\t avg train acc = 99.78%, avg train loss = 0.0081 | test acc = 99.73%, test loss = 0.0112\n",
      "==> epoch 16\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0035\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0049\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0022\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0054\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0033\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0042\n",
      "\t iter 300 \t train acc = 100.00%, train loss = 0.0203\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0117\n",
      "\t avg train acc = 99.78%, avg train loss = 0.0077 | test acc = 99.76%, test loss = 0.0109\n",
      "==> epoch 17\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0033\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0047\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0052\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0031\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0040\n",
      "\t iter 300 \t train acc = 100.00%, train loss = 0.0191\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0111\n",
      "\t avg train acc = 99.79%, avg train loss = 0.0074 | test acc = 99.76%, test loss = 0.0106\n",
      "==> epoch 18\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0032\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0045\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0019\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0050\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0029\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0038\n",
      "\t iter 300 \t train acc = 100.00%, train loss = 0.0180\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0106\n",
      "\t avg train acc = 99.80%, avg train loss = 0.0072 | test acc = 99.80%, test loss = 0.0103\n",
      "==> epoch 19\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0030\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0043\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0017\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0048\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0028\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0036\n",
      "\t iter 300 \t train acc = 100.00%, train loss = 0.0170\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0102\n",
      "\t avg train acc = 99.81%, avg train loss = 0.0069 | test acc = 99.80%, test loss = 0.0101\n",
      "==> epoch 20\n",
      "\t iter 0 \t train acc = 100.00%, train loss = 0.0029\n",
      "\t iter 50 \t train acc = 100.00%, train loss = 0.0042\n",
      "\t iter 100 \t train acc = 100.00%, train loss = 0.0016\n",
      "\t iter 150 \t train acc = 100.00%, train loss = 0.0046\n",
      "\t iter 200 \t train acc = 100.00%, train loss = 0.0026\n",
      "\t iter 250 \t train acc = 100.00%, train loss = 0.0034\n",
      "\t iter 300 \t train acc = 100.00%, train loss = 0.0160\n",
      "\t iter 350 \t train acc = 100.00%, train loss = 0.0098\n",
      "\t avg train acc = 99.82%, avg train loss = 0.0067 | test acc = 99.80%, test loss = 0.0099\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "batch = 32\n",
    "log_freq = 50 # log-printing frequency\n",
    "batches_of_epoch = len(X_train) // batch\n",
    "epochs = 20\n",
    "\n",
    "data_loader = batch_loader(X_train, y_train, batch, shuffle=False)\n",
    "model = simple_classifier(n_in=784, n_out1=120, n_out2=2)\n",
    "sgd = SGD(lr=0.0001) #, momentum=0.8)\n",
    "\n",
    "metrics = [] # to store intermediate results during optimization\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"==> epoch {i+1}\")\n",
    "    \n",
    "    sum_train_acc, sum_train_loss = 0, 0\n",
    "\n",
    "    for itr in range(batches_of_epoch):\n",
    "        # get batch of data\n",
    "        X_train_b, y_train_b = next(data_loader)\n",
    "\n",
    "        params = model.get_params()\n",
    "        train_acc, train_loss = model.forward(X_train_b, y_train_b)\n",
    "        grads = model.backward(X_train_b, y_train_b)\n",
    "        new_params = sgd.update(params, grads, itr)\n",
    "        model.update(new_params)\n",
    "\n",
    "        sum_train_acc += train_acc\n",
    "        sum_train_loss += train_loss\n",
    "\n",
    "        if itr % log_freq == 0:\n",
    "            print(\"\\t iter %d \\t train acc = %.2f%%, train loss = %.4f\" %(itr, train_acc, train_loss))\n",
    "\n",
    "    avg_train_acc, avg_train_loss = sum_train_acc / batches_of_epoch, sum_train_loss / batches_of_epoch\n",
    "\n",
    "    test_acc, test_loss = model.forward(X_test, y_test)\n",
    "    print(\"\\t avg train acc = %.2f%%, avg train loss = %.4f | test acc = %.2f%%, test loss = %.4f\" \n",
    "          % (avg_train_acc, avg_train_loss, test_acc, test_loss))\n",
    "        \n",
    "    metrics += [[avg_train_acc, avg_train_loss, test_acc, test_loss]] # to store intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = np.array(metrics)\n",
    "print(metrics.shape)\n",
    "\n",
    "epochs = len(metrics)\n",
    "x = range(epochs)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.title('accuracy')\n",
    "plt.plot(x, metrics[:,0], 'b-.', label='train')\n",
    "plt.plot(x, metrics[:,2], 'r-*', label='test')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('loss')\n",
    "plt.plot(x, metrics[:,1], 'b-.', label='train')\n",
    "plt.plot(x, metrics[:,3], 'r-*', label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
